## 一个反直觉的事实
如果我告诉你：**GPT-3的训练数据，只用了原始数据的1.27%**，你会不会觉得很意外?
OpenAI从45TB的原始数据中，精选了570GB用于训练GPT-3。他们丢掉了98.73%的数据。而GPT-3的性能，比之前用"更多数据"训练的模型强得多。
这说明什么？**在AI训练中，数据质量比数量重要得多。**
## 为什么不是越多越好？
这和人类学习是一个道理。想象你要学编程，有两种方式：
**方式A**：随机找1万个代码片段来学
这些代码里，有些是初学者写的、bug满天飞
有些是黑客写的、故意混淆让人看不懂
有些是10年前的过时代码、用的库早就不维护了
**方式B**：精选1000个Google工程师写的项目来学
每个项目都有完整注释、架构清晰
都遵循最佳实践、错误处理得当
都是近两年的代码、用的是现代语言特性
你觉得哪种方式学得更好？显然是方式B。
AI训练也是一样。**喂给它垃圾代码，它就学会写垃圾代码。**
最近一项研究发现：当训练数据中有过度重复时，AI模型的准确率会下降40%。而另一个数据更触目惊心：85%的AI项目失败，主要原因是数据质量差。
## 什么算"高质量代码"？
在AI训练领域，高质量代码有几个关键特征：
### 可读性强
变量命名有意义——`user_count` 而不是 `x`
结构清晰、模块化好
有注释，但不是废话注释
为什么这重要？因为AI需要"理解"代码在做什么。如果代码本身就难以理解，AI学到的就是"混乱的逻辑"。它会认为"写代码就该这么乱"。
### 有完整上下文
不是孤立的代码片段，而是完整项目
有README、文档说明
有测试用例——这能让AI理解"正确的输出应该是什么"
很多开源项目只有代码没有文档。AI看到 `def process(data)` 这样的函数，完全不知道`data`应该是什么格式、函数要做什么、返回值是什么意思。
但有文档的代码不一样。AI能建立"输入→处理→输出"的完整理解。它知道为什么要这样写，而不是仅仅模仿"怎么写"。
### 遵循最佳实践
符合该语言的编程规范（Python的PEP 8，JavaScript的Airbnb规范）
使用成熟的设计模式
错误处理得当
性能优化合理
如果AI学了大量"能跑但不优雅"的代码，它就会生成能跑但低效的代码。而学习了经过Code Review、经过优化的代码，它会生成更专业的代码。
### 新鲜度
使用现代的语言特性（Python 3.10+ 的特性，而不是Python 2的写法）
依赖的库是主流且维护良好的
没有过时的API调用
这点很关键。如果训练数据里有大量2010年的jQuery代码，AI可能建议你用jQuery——但现在前端开发早就转向React/Vue了。AI会给出"技术上正确但时代上过时"的建议。
## 低质量代码的危害：一个具体例子
让我举个例子。假设训练数据里有这段代码：
```python
def calc(a,b):
    return eval(str(a)+"+"+str(b))
```
这代码"能用"——它确实能算加法。但它有严重问题：
`eval`有安全风险——如果用户输入是恶意代码呢？
转字符串再eval完全多余——直接`a+b`就行
变量名没意义——`a`和`b`是什么？
如果AI学了大量这样的代码，它会认为"这是正常写法"。然后当用户让它写一个计算函数时，它就生成有安全漏洞的代码。
用户运行了这段代码，被黑客攻击了。这不是AI"太笨"，而是它学到的就是"错误的范式"。
## Claude可能的数据策略
Anthropic很可能做了这些事情（虽然他们没公开细节）：
**精选数据源**
只从Star数高、维护活跃的GitHub项目抓取
可能特别关注某些知名公司的开源项目（Google、Meta、Microsoft等）
筛选有完整CI/CD、测试覆盖率高的项目
**数据清洗**
过滤掉明显的bug代码
去除重复代码——很多项目会复制粘贴同样的代码，这对训练没帮助
可能用静态分析工具扫描，去除有已知安全问题的代码
**数据标注和增强**
可能人工标注了一些"优秀代码示例"
可能让AI先学习"文档+代码"的配对，而不是只学代码本身——这样AI理解的是"为什么这样写"，而不是"怎么写"
**配比优化**
不是均匀分配各语言，而是根据"教学价值"分配
比如Python的科学计算库、JavaScript的前端框架、Rust的系统编程——这些代码的"信息密度"更高
可能减少了过时语言（如Perl、PHP老版本）的占比
## GPT为什么没这么做？
不是做不到，而是**策略不同**。
OpenAI的目标是"通用AI"。它想在对话、写作、图像、编程等所有领域都"够用"。如果在代码数据上花太多精力筛选，就要减少其他领域的数据量或处理时间。
而且ChatGPT的用户群体更广泛——不是主要面向程序员。有人用它写诗、有人用它做旅行规划、有人用它解释历史事件。OpenAI不能只为了"编程能力最强"而牺牲其他能力。
这就是设计理念的差异：Claude说"我要在编程上做到最好"，GPT说"我要在所有事情上都够用"。
## 写作能力的对比：没有编程那么悬殊
讲完编程，你可能会想：**那写作能力呢？Claude和GPT最新版哪个更强？**
答案是：**没有编程那么悬殊，各有优势，取决于写作类型。**
### Claude的写作优势
**结构化思考能力更强**——处理需要深度逻辑的写作时表现更好，比如学术论文、技术文档、长篇分析。
为什么？可能是训练数据的差异。就像编程数据一样，Anthropic可能对写作训练数据也做了筛选：更多来自学术期刊、专业出版物，更少"水文"、营销软文、低质量博客。
**更擅长"层层递进"的叙事**——Claude写长文时，逻辑链条特别清晰：从问题引入 → 解释原理 → 深入分析 → 回答"为什么" → 提出新问题。这种"钩子式叙事"，Claude做得更自然。
**更"人性化"的语言风格**——Claude倾向于用对话感、设问、转折词（"但是"、"你可能会想"、"这里有个有趣的点"）。GPT有时候会写得更"工整"、"正式"，但缺少人的味道。
### GPT的写作优势
**创意和发散能力更强**——如果让GPT写故事、写比喻、写营销文案，它往往更"灵动"、更"跳跃"。
为什么？因为GPT的训练数据覆盖更广——它见过海量的文学作品、广告文案、社交媒体内容。它的"词汇库"和"表达方式库"更多元。
**更适合"轻松幽默"的内容**——让GPT写段子、写幽默的解释、写轻松的科普，它会更"接地气"。Claude写幽默？有时候会显得"用力过猛"，或者太正经了。
**更灵活的风格切换**——GPT能快速切换风格，从严肃学术到搞笑段子，从诗歌到技术文档。Claude在风格切换上稍微"笨重"一点，有时候会保留一些"Claude式"的痕迹。
### 一个关键差异：写作没有"绝对标准"
编程能力可以客观衡量——代码能不能跑、bug多不多、性能好不好。
但写作呢？**没有绝对的"好"与"坏"**，只有"适不适合场景"。
- 写技术博客、深度分析 → Claude可能更合适
- 写小红书文案、创意故事 → GPT可能更合适
而且，很多时候人的判断标准也不一样。有人喜欢Claude的严谨，有人觉得它太"刻板"；有人喜欢GPT的灵动，有人觉得它太"浮夸"。
## 如果要做写作Agent，怎么保持"人的主体性"？
从编程到写作，我们看到了一个共同点：**数据质量影响AI能力**。
但这里有个更深的问题：既然AI写作能力越来越强，**如何设计一个写作系统，让人保持主体性，而不是变成"AI代写"？**
### 现有AI写作工具哪里错了？
市面上大部分AI写作工具，本质上是这样的：
用户："帮我写一篇关于XX的文章"
AI：哗啦哗啦写了3000字
用户："改一下这里"
AI：改完了
**问题在哪里？**
1. **AI是生产者，人是审核者**——主客体颠倒了
2. **人失去了思考过程**——你不知道AI为什么这么写，你也没参与构思
3. **内容是AI的，不是你的**——你只是在"检查作业"，不是在"创作"
这就像：你想学做菜，但AI直接把菜做好了，你只负责尝一口、提点意见。**你永远学不会做菜，而且你吃的也不是"你的菜"。**
### 什么叫"以人为主体性"？
真正的"以人为主体性"，应该满足这几个特征：
**人控制「是什么」，AI优化「怎么说」**
- 错误：用户说"写一篇关于Transformer的文章"，AI决定讲什么、怎么讲、结构是什么
- 正确：用户自己想清楚要讲"Transformer为什么快"，自己构思逻辑，自己写出核心观点骨架，AI帮你扩展成流畅的语言、补充例子、优化表达
**关键**：内容的"灵魂"（观点、逻辑、结构）是人的，AI只是帮你把"灵魂"表达得更好。
**人掌握知识，AI激发思考**
很多人写不出东西，不是因为"不会写"，而是因为"想不清楚"。
- 错误：用户说"我不知道写什么"，AI说"那我帮你写吧"
- 正确：用户说"我对Transformer有些模糊的理解，但说不清楚"，AI通过对话、提问，激发用户思考，用户在对话中逐渐想清楚，然后AI帮你把思考总结成文字
**关键**：知识是你的，AI只是帮你**显化**出来。就像雕刻家说的："雕像本来就在石头里，我只是把多余的部分去掉。"
**人决定结构，AI提供选项**
写作的核心是结构——先讲什么、后讲什么、怎么递进。这必须是人决定的。
- 错误：AI说"我生成了一个结构，你看行不行？"用户："好像还行"（但其实没深入思考过）
- 正确：AI说"你有3张卡片：A讲原理、B讲应用、C讲局限。我看到3种排布方式：方案1（A→B→C）、方案2（B→A→C）、方案3（C→A→B）。你觉得哪种符合你的思路？"用户自己选择方案2，因为"我想让读者先看到效果，再理解原理"
**关键**：AI提供选项，但**选择权在人手里**。人在选择的过程中，也在深化自己的思考。
**人迭代内容，AI执行细节**
写作是迭代的——写完第一版，发现不够好，再改、再优化。
- 错误：用户说"这段不太对，你帮我重写"，AI重写了，但用户不知道改了什么、为什么改
- 正确：用户说"这段感觉太平铺直叙了，缺少悬念"（人提出方向），AI说"我理解，你想增加悬念感。我可以这样改：方案1（先抛出反直觉事实）、方案2（用问题开头，制造好奇）。你倾向哪种？"用户选方案1（人决定方向），AI执行修改（AI执行细节）
**关键**：人说"要什么"，AI说"怎么做"。
### 一个具体的设计：4阶段工作流
基于上面的理念，一个"以人为主体性"的写作Agent应该这样设计：
**阶段1：激发和摄入（Learning）**
目标：让人想清楚、掌握知识
流程：AI提问 → 用户回答 → AI深挖 → 用户澄清 → AI总结 → 生成知识卡片
关键：AI不要直接给答案，而是问问题。用户在回答的过程中，自己想清楚。知识卡片的内容，是用户说的话的提炼，不是AI编的。
类比：就像心理咨询师——不是告诉你答案，而是通过提问让你自己找到答案。
**阶段2：结构规划（Structure）**
目标：让人决定"讲什么、怎么排布"
流程：AI全局分析 → AI提供方案（2-3个选项）→ 用户选择 → AI搭建结构 → AI生成首版
关键：AI不要直接决定结构，而是提供选项。用户必须做选择，不能只是"嗯嗯好"。选择的过程，就是用户在深化自己的思路。
类比：就像建筑师给你看3种户型图，你选一个，然后他帮你施工。
**阶段3：迭代优化（Iteration）**
目标：让人把控方向，AI执行细节
流程：用户提出问题 → AI深入讨论（不直接改，而是和用户对话）→ 用户说"整理" → AI整理内容 → AI润色
关键：不要一改就改，而是先对话、后整理。对话的目的是让用户想清楚，而不是AI直接给答案。整理的内容，是用户在对话中的思考，不是AI自己编的。
类比：就像导演和演员对戏——先讨论这场戏的情绪、动机，演员想清楚了，再正式拍摄。
**阶段4：最终成稿（Finalization）**
目标：让人确认最终内容
流程：AI读取所有卡片 → AI推断顺序 → 用户确认 → AI串联生成 → 用户最终审核
关键：顺序必须让用户确认，不能AI自己决定。串联不是简单拼接，而是要有过渡、呼应、递进。最终内容的灵魂，仍然是用户在学习阶段想清楚的东西。
### 核心设计原则：AI永远不做"决定"
你可能发现了，这个设计的核心是：**AI永远不做"决定"，只做"执行"和"建议"。**
| 环节 | 人的职责 | AI的职责 |
|------|---------|---------|
| 想清楚内容 | 思考、回答、澄清 | 提问、激发、总结 |
| 决定结构 | 选择方案、排布顺序 | 提供选项、分析关系 |
| 把控方向 | 提出问题、确认方向 | 讨论、提供方案 |
| 优化语言 | 确认是否符合意图 | 润色、扩写、调整 |
| 最终审核 | 决定是否发布 | 串联、生成、修改 |
**关键规律**：
- "是什么"永远是人决定——AI只能建议，不能替你决定
- "怎么说"AI可以优化——但必须符合人的意图
- "为什么"必须人想清楚——AI只是帮你显化出来
这样设计的结果是：你掌握知识（因为你在学习阶段想清楚了）、你掌握结构（因为你在结构阶段选择了）、你掌握内容（因为你在迭代阶段把控了方向）。最终文章是"你的"，不是AI代写的。
## 一个更深的问题
现在你理解了：同样是Transformer架构，Claude的编程能力更强，是因为从训练数据到微调策略，每个环节都针对性优化。而数据质量，比数据数量更关键。
在写作领域，Claude和GPT的差异没有编程那么悬殊，因为写作没有"绝对标准"，取决于场景和审美。
但这里有个更深的问题：既然Claude和GPT的训练策略不同，那它们在**实际使用**中的表现，会不会也不一样？
理论上，Claude应该在所有编程场景都更强。但真的是这样吗？
如果你在实际使用Cursor这类编程工具时，会发现一个有趣的现象：Claude在架构设计、探索方案时确实很强，但在debug时，反而不如GPT——"怎么说都改不对"，而GPT"说完就懂，就改对了"。
为什么理论优势在实际场景中会反转？这背后又是什么原因？